package selection

import (
	"bufio"
	"encoding/json"
	"fmt"
	"github.com/denverquane/redditcommentanalysis/filesystem"
	"github.com/valyala/fastjson"
	"io/ioutil"
	"log"
	"net/http"
	"os"
	"regexp"
	"strconv"
	"strings"
	"time"
)

//TODO explore speedups related to unravelling the recursion
func recurseBuildCompleteLine(reader *bufio.Reader) []byte {
	line, isPrefix, err := reader.ReadLine()
	if err != nil {
		log.Println(err)
		return nil
	}

	if isPrefix {
		return append(line, recurseBuildCompleteLine(reader)...)
	} else {
		return line
	}
}

func getCommentDataFromLine(line []byte, keyTypes map[string]string) map[string]string {
	result := make(map[string]string, 0)
	filter, _ := regexp.Compile("[^a-zA-Z0-9 ']+")

	for key, v := range keyTypes {
		if v == "str" {
			if key == "body" {
				result[key] = filter.ReplaceAllString(filterStopWordsFromString(strings.ToLower(fastjson.GetString(line, key))), "")
				result["sentiment"] = strconv.FormatFloat(GetSentimentForString("http://192.168.1.192:8888", result[key]), 'f', 10, 64)
			} else {
				result[key] = strings.ToLower(fastjson.GetString(line, key))
			}
		} else if v == "int" {
			result[key] = strconv.Itoa(fastjson.GetInt(line, key))
		} else {
			log.Fatal("Undetected type: " + v)
		}
	}
	return result
}

func GetSentimentForString(url, text string) float64 {
	// Generated by curl-to-Go: https://mholt.github.io/curl-to-go
	body := strings.NewReader(`text=` + text)
	req, err := http.NewRequest("POST", url, body)
	if err != nil {
		// handle err
		log.Fatal(err)
	}
	req.Header.Set("Content-Type", "application/x-www-form-urlencoded")

	resp, err := http.DefaultClient.Do(req)
	if err != nil {
		fmt.Println("Input: " + text)
		log.Println(err)
		return GetSentimentForString(url, text)
	}
	retbody, err := ioutil.ReadAll(resp.Body)
	if err != nil {
		log.Printf("Error reading body: %v", err)
		return 0
	}

	defer resp.Body.Close()
	f64 := fastjson.GetFloat64(retbody, "comparative")
	return f64
}

const percentPerMonth = (1.0 / 12.0) * 100.0

func ExtractCriteriaDataToFile(criteria []Criteria, year, month, basedir string, schema commentSchema, progress *float64) []int {
	var commentsInRawFile int64

	filesystem.CreateSubdirectoryStructure(basedir, month, year)

	outputFilePaths := make([]string, len(criteria))
	outputFileWriters := make([]*os.File, len(criteria))
	results := make([]int, len(criteria))
	extractedCommentCounts := make([]int64, len(criteria))

	rawDataFilePath := basedir + "/RC_" + year + monthToIntString(month)
	commentsInRawFile = readInCommentCountMetadata(rawDataFilePath)

	for i, v := range criteria {
		v.Value = strings.ToLower(v.Value)
		v.Test = strings.ToLower(v.Test)
		criteria[i] = v
		outputFilePaths[i] = basedir + "/Extracted/" + year + "/" + month + "/" + v.Test + "_" + v.Value + "_" + schema.name
		if _, err := os.Stat(outputFilePaths[i]); !os.IsNotExist(err) {
			fmt.Println("Found cached data for " + outputFilePaths[i])
			file, _ := os.Open(outputFilePaths[i])
			results[i], err = LineCounter(file)
			continue
		} else {
			fmt.Println("No cached data found for " + outputFilePaths[i])
			results[i] = 0
		}

		//Note: Not deferring a outputfile close, because we need to close/rename when extraction is done
		outputFileWriter, e := os.Create(outputFilePaths[i] + ".tmp")
		if e != nil {
			log.Println(e)
		}
		outputFileWriters[i] = outputFileWriter
	}

	rawDataFile, fileOpenErr := os.Open(rawDataFilePath)
	if fileOpenErr != nil {
		fmt.Print(fileOpenErr)
		os.Exit(0)
	} else {
		fmt.Println("Opened file " + rawDataFilePath)
	}

	rawDataFileReader := bufio.NewReaderSize(rawDataFile, 4096)

	var linesRead int64 = 0
	startTime := time.Now()
	tempTime := startTime

	allCached := true
	for _, v := range results {
		if v == 0 {
			allCached = false
		}
	}

	if !allCached {
		for {
			line := recurseBuildCompleteLine(rawDataFileReader)
			if line == nil {
				fmt.Println("Lines: " + strconv.FormatInt(linesRead, 10))
				log.Println("Encountered error; concluding analysis")
				break
			}
			linesRead++

			for i, v := range criteria {
				if results[i] == 0 {
					if strings.Contains(strings.ToLower(string(line)), "\""+v.Test+"\":\""+v.Value+"\"") {
						parsed := getCommentDataFromLine(line, schema.schema)
						marshalled, e := json.Marshal(parsed)
						if e != nil {
							log.Println(e)
						} else {
							outputFileWriters[i].Write(marshalled)
							outputFileWriters[i].Write([]byte("\n"))
							extractedCommentCounts[i]++
						}
					}
				}
			}

			if linesRead%HundredThousand == 0 {
				progressStr := ""
				if commentsInRawFile != 0 {
					*progress = (float64(linesRead) / float64(commentsInRawFile)) * 100.0
					progressStr = strconv.FormatFloat(*progress, 'f', 2, 64) + "%"
				} else {
					progressStr = strconv.FormatInt(linesRead, 10) + " lines total"
				}
				fmt.Println("Processed 100k lines in " + time.Now().Sub(tempTime).String() + " (" + progressStr + ")")
				tempTime = time.Now()
			}
		}
	}
	dif := time.Now().Sub(startTime).String()
	tempStr := "Took " + dif + " to search " + strconv.FormatInt(linesRead, 10) + " comments of file " + rawDataFilePath + "\n"
	fmt.Println(tempStr)

	for i, v := range criteria {
		if results[i] == 0 {
			if extractedCommentCounts[i] == 0 {
				log.Println("Found 0 comments for " + v.Test + ":" + v.Value + " in " + month + ", exiting extraction!")
				results[i] = 0
			} else {
				log.Println("Extracted " + strconv.FormatInt(extractedCommentCounts[i], 10) + " comments for " +
					v.Test + " = " + v.Value + " in " + month + "/" + year)
			}

			outputFileWriters[i].Close()
			err := os.Rename(outputFilePaths[i]+".tmp", outputFilePaths[i])
			log.Println(err)
		}
	}
	*progress = 100.0
	if commentsInRawFile == 0 {
		log.Println("Never read the linecount from a file; writing to file now")
		f, err := os.Create(rawDataFilePath + "_count")
		if err != nil {
			log.Println(err)
		}
		f.Write([]byte(strconv.FormatInt(int64(linesRead), 10)))
		f.Close()
	}

	return results
}
